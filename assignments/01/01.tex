%	
%	01.tex - Week 1, deadline on 3th of May 2013
%	
%	This document provides answers to the exercises and problems as defined by
%	the course for the week shown below.
%	
%	Mandatory:	2-2
%	Optional:	1.1-3, 1.1-5, 2.2-2, 2.3-4, 3.1-4, 3.2-7, 3-2
%	Extras:		1.1-1, 1.1-2, 1.1-4, 1.2-1, 1.2-2
%	

\documentclass[11pt,english]{article}

\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{amsmath,amssymb}	% for mathematical notation
\usepackage[linesnumbered]{algorithm2e}

%========== meta data ==========%

\title
{
	\vspace{1in}
	Algorithms \& Datastructures\\
	\huge Assignment 1
}

\author
{
	Casper B. Hansen\\
	\small Department of Computer Science\\
	\small The University of Copenhagen\\
	\texttt{fvx507@alumni.ku.dk}
	\and
	Hans J. T. Stephensen\\
	\small Department of Computer Science\\
	\small The University of Copenhagen\\
	\texttt{xkv467@alumni.ku.dk}
}

\date{\today}


%========== settings ==========%

\setlength{\headheight}{15pt}
\sectionfont{\Large}


%========== macros ==========%

% no macros yet


%========== document ==========%

\begin{document}

\clearpage
\maketitle
\thispagestyle{empty}

%========== mandatory ==========%

\newpage
\pagestyle{fancy}

\section*{Mandatory Hand-ins}

\subsection*{2-2 Correctness of bubblesort}
Bubblesort is a popular, but inefficient, sorting algorithm. It works by
repeatedly swapping elements that are out of order.
\\\\
\begin{algorithm}[H]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\SetKwFunction{Exchange}{Exchange}
	\SetKw{KwDownTo}{down to}
	
	\Input{An unsorted array $A$.}
	\Output{A sorted array $A$ (sorts in place).}
	\BlankLine
	\For{$i = 1$ \KwTo $A.length - 1$}
	{
		\For{$j = A.length$ \KwDownTo $i + 1$}
		{
			\If{$A[j] > A[j-1]$}
			{
				exchange $A[j]$ with $A[j-1]$
			}
		}
	}
	\caption{Bubblesort}
\end{algorithm}

\subsubsection*{\large a. \mdseries Let $A'$ denote the output of bubblesort(A). To
prove that bubblesort is correct, we need to prove that it terminates and that
\[A'[1] \leq A'[2] \leq \dots \leq A'[n]\text{, where $n = A.length$.}\] To
show that Bubblesort actually sorts, what else do we need to prove?}
First, we must determine and state the loop invariants, and prove that these
loop invariants hold during initialization phase (prior to the first iteration,
that is), and that this remains true as maintained by the algorithm throughout
the execution of each iteration of the loops, which is called the maintainance
phase, until the termination phase.

\subsubsection*{\large b. \mdseries State precisely a loop invariant for the
\textbf{for} loop in lines 2-4, and prove that this loop invariant holds. Your
proof should use the structure of the loop invariant proof presented in this
chapter.}
For the inner for-loop, we state the following loop-invariant.
\[A[j] = min\{A[j \cdots n]\}]\]
\textbf{Initialization} \mdseries On entry the value of $j$ is equal to the
length of the array $A$, that is $j = n$. The range $A[j \dots n]$ is
trivially sorted, as it holds only one element, which is $A[j]$ - hence the
loop invariant holds.
\\\\
\textbf{Maintainance} \mdseries By induction, we need to show, assuming $A[j]
= min\{A[j \cdots n]\}$ holds, that after iteration, $A[j-1] = min\{
A[j-1 \cdots n]\}$. During the body of the inner for-loop, the
if-statement and its body ensures that $A[j-1] = min\{A[j-1],A[j]\}$ and
$A[j] = max\{A[j-1],A[j]\}$. Notice that the state of the array $A$ is
unchanged. Furthermore, using the above $A[j-1] = min\{A[j-1],A[j]\}
= min\{A[j-1], A[j \cdots n]\} = min\{A[j-1 \cdots n]\}$.
The wanted has been shown.
\\\\
\textbf{Termination} \mdseries The loop is terminated when $j = i$. That is,
$j$ is decreased on every iteration of the loop, and when it gets to
$j = i$ the for-loop then evaluates the condition to false and execution
leaves the for-loop.

At termination, as proved in maintainance, $A[j]$ holds $min\{A[i \dots n]\}$,
placing the minimum element at $A[j - 1] = A[i + 1 - 1] = A[i]$.

\subsubsection*{\large c. \mdseries Using the termination condition of the loop
invariant proved in part (b), state a loop invariant for the \textbf{for} loop
in lines 1-4 that will allow you to prove the inequality. Your proof should
use the structure of the loop invariant proof presented in this chapter.}
For the outer for-loop it holds that $A[1 \dots i-1]$ is sorted. That is
$max{A[1 \dots i - 1]} \leq min\{A[i \dots n]\}$, and $A[1 \dots i-1]$ is
in sorted order.
\\\\
\textbf{Initialization} \mdseries On entry $i = 1$, and as the loop invariant
states $A[1 \dots i - 1]$ should appear in sorted order. However, since
$i = 1$ on the first pass, the range isn't valid and therefore there can be
no elements in the array, leaving us with an empty array, which is trivially
sorted.
\\\\
\textbf{Maintainance} \mdseries Assuming $ A[1] \leq A[2] \leq \cdots \leq 
A[i-1] \leq min\{ A[i \cdots n] \}$ we want to show that after the iteration,
$ A[1] \leq A[2] \leq \cdots \leq A[i-1] \leq A[i] \leq min\{ A[i+1 \cdots n]
\}$. During the iteration, the inner for-loop makes $A[i] = min\{ A[i \cdots 
n] \} \leq min\{ A[i+1 \cdots n] \}$. Substituting into the above, we get $ 
A[1] \leq A[2] \leq \cdots \leq A[i-1] \leq A[i] = min\{ A[i \cdots n] \} 
\leq min\{ A[i+1 \cdots n] \}$. Which is what we wanted.
\\\\
\textbf{Termination} \mdseries The outer for-loop terminates with $i = n-1$,
placing $min\{A[i \cdots n] = A[n-1 \cdots n]\}$ on place $A[i]$, ensuring 
that $ A[1] \leq A[2] \leq \cdots \leq A[i] = A[n-1] \leq A[n]$. The array 
is now sorted.

\subsubsection*{\large d. \mdseries What is the worst-case running time of
bubblesort? How does it compare to the running time of insertion sort?}
Usually one can deduce a rough notion of the running time by the amount of
nested loops, and from this we estimate that the running time of bubblesort
is $\Theta(n^2)$.

This is, however, not an accurate way of analysing the complexity of the
algorithm - even though it may be true for this particular algorithm.

The 'brute-force' way of analysing an algorithm is to determine the costs of
each line in the algorithm and produce a formula of the summation of all of
these costs. From this we can reduce the expression, using the rules from
asympotic notation, and end up with the exact order it belongs to. Doing this
we have that line 1 gets executed $n$ times, line 2 gets executed
$\sum^{n}_{j=1}{j}$ times, and in the worst case both of lines 3 and 4 gets
executed $\sum^{n-1}_{j=1}{j}$ times, respectively.
\begin{align}
	T(n) &= c_1 n + c_2 \sum^{n}_{j = 1}{j}
	+ (c_3 + c_4)\sum^{n - 1}_{j = 1}{j}\\
	&=
	c_1 n + c_2 \frac{n(n + 1)}{2}
	+ (c_3 + c_4)\frac{n(n - 1)}{2}\\
	&=
	c_1 n + c_2 \frac{1}{2} n^2 + c_2 \frac{1}{2} n +
	(c_3 + c_4) \frac{1}{2} n^2 - (c_3 + c_4) \frac{1}{2} n \\
	&=
	(c_2 + c_3 + c_4) \frac{1}{2} n^2 +
	(2 c_1 + c_2 - c_3 - c_4) \frac{1}{2} n
\end{align}
Notice that constant terms have been isolated, and according to asymptotic
analysis leading constants are simply dropped. Furthermore the term $n^2$ is
of higher order than $n$, and we can then conclude that $T(n) = O(n^2)$.

Even in the best case both loops have to run exactly the same amount of times,
from which we can then say that the lower bound is $T(n) = \Omega(n^2)$. And,
since upper- and lower bound are asymptotically equal, then
$T(n) = \Theta(n^2)$.


%========== optional ==========%

% no optional hand-ins


%========== extras ==========%

\newpage
\section*{Extras}

\subsection*{1-1 Comparison of running times}
For each function $f(n)$ and time $t$, determine the size $n$ of a problem
that can be solved in time $t$, assuming that the algorithm to solve the
problem takes $f(n)$ microseconds.
\\\\
\small
\begin{tabular}[]{|c|c|c|c|c|c|c|c|}
	\hline
	{\ } & 1 second & 1 minute & 1 hour & 1 day & 1 month & 1 year & 1 century\\
	\hline
	
	lg$n$ & $2^{10^6}$ & $2^{10^6 \cdot 60}$ & $2^{10^6 \cdot 60^2}$ &
	$2^{10^6 \cdot 60^2 \cdot 24}$ & $2^{10^6 \cdot 60^2 \cdot 24 \cdot 365}$ &
	$2^{10^6 \cdot 60^2 \cdot 24 \cdot 365}$ &
	$2^{10^6 \cdot 60^2 \cdot 24 \cdot 365 \cdot 100}$\\	
	
	$\sqrt{n}$ & $10^{12}$ & . & . & . & . & . & .\\
	
	$n$ & . & . & . & . & . & . & .\\ 
	
	$n$lg$n$ & . & . & . & . & . & . & .\\ 
	
	$n^2$ & . & . & . & . & . & . & .\\ 
	
	$n^3$ & . & . & . & . & . & . & .\\ 
	
	$2^n$ & . & . & . & . & . & . & .\\ 
	
	$n$! & . & . & . & . & . & . & .\\ 
	
	\hline
\end{tabular}

%========== exercises ==========%

\newpage
\pagestyle{fancy}

\section*{Exercises}

\subsection*{1.1-1 \mdseries Give a real-world example that requires sorting
or a real-world example that requires computing a convex hull.}
In a graphics engine that gives depth-perception, one will usually sort the
objects in the scene according to their depth and draw the objects in that
order, such that no object further away from another gets drawn on top of the
latter object.

% TODO: Convex hull example

\subsection*{1.1-2 \mdseries Other than speed, what other measures of
efficiency might one use in a real-world setting?}
Apart from speed there are numerous other factors of efficiency to account for
when writing an algorithm. Some of these factors may or may not be relevant for
a particular case, but in general memory consumption is among the most
prevalent concerns these days, even though memory has gotten quite large. The
reason being that we still demand more and more of our computers, so this will
likely always be an area of interest. Security is one of the most discussed
topics of today, and this affects algorithm design as well (ie. one algorithm
may expose vulnerabilities that another may account for).

\subsection*{1.1-3 \mdseries Select a datastructure that you have seen
previously, and discuss its strengths and weaknesses.}
A linked list provides the advantage of fast manipulation (ie. insertion,
deletion, etc.), but at the cost of search time. Using a regular array, since
it is simply a pointer pointing at the address of the first block of the
allocated area of memory, it is a trivial operation to read or write to any
particular offset in memory of a regular array, either by indexing or pointer-
arithmetic.

This is not true for a linked list, as we have to search through the entire
list (in the worse case) to find any particular index.

\subsection*{1.1-4 \mdseries How are the shortest-path and traveling-salesman
problems given above similar?}
The similarity of the two problems are quite obvious. Given any routing
problem, finding the shortest path will yield a faster route, for the
traveling-salesman this means that we calculate the shortest intermediate
paths for each destination, and make each destination a node of its own in a
new graph, where we can then determine the shortest possible route through
these whilst ending up in the starting position.

\subsection*{1.1-5 \mdseries Come up with a real-world problem in which only
the best solution will do. Then come up with one in which a solution that is
"approximately" the best is good enough.}
First rule of development is to make things work, then optimize. As such, we
will usually be slobby in our algorithmic designs on the first iterations of
any software project, and later optimize it for speed - this may or may not
require the very best solution, but preferably it should be.

For a concrete example; if part of a system deals with security, then it is
imperative that the solution be carried out with the utmost care to every
conceivable detail. This is a careful balance between usability, vulnerability
and performance, and as such only the very best solution must be employed.

Contrary to this are trivial parts of a system that doesn't directly deal with
anything that is visible to the user of the system, or is affected by it
during use of the system. Like an automated backup service, running in the
background. The only thing to ensure here is that it performs its task
accurately, but beyond that performance is not an issue - being a background
task we would actually like it not to consume too much CPU time, so we may
actually hinder it from taking up CPU-cycles when other user-owned processes
are in need of system resources.

\subsection*{1.2-1 \mdseries Give an example of an application that requires
algorithmic content at the application level, and discuss the function of the
algorithms involved.}
Any application that sorts content, like say a photo application, will have
at least one sorting algorithm (ie. sort by date, sort by location, etc.).
In such an algorithm we would convert the data into a comparable type, like
an integer, and sorting then becomes trivial.

\subsection*{1.2-2 \mdseries Suppose we are comparing implementations of
insertion sort and merge sort on the same machine. For inputs of size $n$,
insertion sort runs in $8n^2$ steps, while merge sort runs in $64n$lg$n$
steps. For which values of $n$ does insertion sort beat merge sort?}

\subsection*{2.2-1 \mdseries Express the function
$n^3/1000 - 100n^2 - 100n + 3$ in terms of $\Theta$-notation.}
With a bit of rearranging of the expression, we get
\begin{align}
	\frac{1}{1000} n^3 - 100n^2 - 100n + 3
\end{align}
and we then see that if we drop the leading constants, which doesn't make up
any significant contribution to the overall growth of the function, we then
get
\begin{align}
	n^3 - n^2 - n
\end{align}
We can now drop the lower order terms, as their growth are insignificant
compared to the highest order term, and we end up with the answer
\begin{align}
	\Theta(n^3)
\end{align}

\subsection*{2.2-2 \mdseries Consider sorting $n$ numbers stored in array $A$
by first finding the smallest element of $A$ and exchanging it with the
element in $A[1]$. Then find the second smallest element of $A$, and exchange
it with $A[2]$. Continue in this manner for the first $n-1$ elements of $A$.
Write pseudocode for this algorithm, which is known as
\textbf{selection sort}. What loop invariant does this algorithm maintain?
Why does it need to run for only the first $n-1$ elements, rather than for all
$n$ elements? Give the best-case and worst-case running times of selection
sort in $\Theta$-notation.}
...


\subsection*{3.1-4 \mdseries ...}
...

\subsection*{3.2-7 \mdseries ...}
...

\subsection*{3-2 \mdseries ...}
...


\end{document}

